{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pytz import timezone\n",
    "from datetime import datetime\n",
    "from config import *\n",
    "from utils import *\n",
    "from import_data import Import_EfficientNetB7_data, Import_TripletNet_test_data\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from model.EfficientNetB7 import EfficientNetB7_model\n",
    "from model.TripletNet import TripletNet_model\n",
    "from tensorflow.keras.models import Model, load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "if len(physical_devices) > 0:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = os.path.join(CHECKPOINT_PATH, os.listdir(CHECKPOINT_PATH)[-1])\n",
    "if checkpoint_path:\n",
    "    print(f\"Checkpoint found: {checkpoint_path}\")\n",
    "    model = load_model(checkpoint_path)\n",
    "else:\n",
    "    print(\"No checkpoint found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODEL_NAME == 'TripletNet':\n",
    "\ttrain_triplet_generator, test_triplet_generator = Import_TripletNet_test_data(IMAGE_SIZE, BATCH_SIZE, train_data_path=TRAIN_DATA_PATH, test_data_path=TEST_DATA_PATH).build_generators()\n",
    "\tmodel = model.get_layer('model')\n",
    "\tmodel = Model(inputs=model.input, outputs=model.output)\n",
    "    \n",
    "elif MODEL_NAME == 'EfficientNetB7':\n",
    "\ttest_generator = Import_EfficientNetB7_data(IMAGE_SIZE, BATCH_SIZE, test_data_path=TEST_DATA_PATH).build_generators('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_triplet_model(test_triplet_generator, database, model, output_path):\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    total_batches = len(test_triplet_generator)\n",
    "    batch_count = 0\n",
    "\n",
    "    print(\"Starting to evaluate batches...\")\n",
    "    for batch, labels in test_triplet_generator:\n",
    "        if batch_count >= total_batches:\n",
    "            break \n",
    "        batch_count += 1 \n",
    "        print(f\"Processing batch {batch_count}/{total_batches}\") \n",
    "        embeddings = model.predict(batch, verbose=0) # (batch, 128)\n",
    "        labels = labels.astype(int) # (batch, )\n",
    "        for emb, true_label in zip(embeddings, labels):\n",
    "            pred_label, _ = predict_closest_embedding(emb, database)\n",
    "            y_true.append(true_label)\n",
    "            y_pred.append(pred_label)\n",
    "    \n",
    "    print(\"Saving results...\")\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average='macro')\n",
    "    recall = recall_score(y_true, y_pred, average='macro')\n",
    "\n",
    "    conf_mat = confusion_matrix(y_true, y_pred)\n",
    "    df_conf_mat = pd.DataFrame(conf_mat, columns=[str(i) for i in range(conf_mat.shape[0])],\n",
    "                            index=[str(i) for i in range(conf_mat.shape[1])])\n",
    "    sns_heatmap = sns.heatmap(data=df_conf_mat, annot=True, fmt='d', linewidths=.5, cmap='BuGn_r')\n",
    "    sns_heatmap.get_figure().savefig(f\"{output_path}/confusion_matrix.png\")\n",
    "\n",
    "    target_names = [str(i) for i in range(conf_mat.shape[0])]\n",
    "    report = classification_report(y_true, y_pred, digits=5, target_names=target_names)\n",
    "\n",
    "    with open(f\"{output_path}/result.txt\", \"w\") as file:\n",
    "        file.write(f\"test_accuracy: {accuracy}, test_precision: {precision}, test_recall: {recall}\\n\")\n",
    "        file.write(report)\n",
    "\n",
    "    print(f\"test_accuracy: {accuracy}, test_precision: {precision}, test_recall: {recall}\")\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(TEST_RESULT_FILE_PATH):\n",
    "        os.makedirs(TEST_RESULT_FILE_PATH) \n",
    "\n",
    "if MODEL_NAME == 'TripletNet':\n",
    "    database = create_embedding_database(train_triplet_generator, model)\n",
    "    evaluate_triplet_model(test_triplet_generator, database, model, TEST_RESULT_FILE_PATH)\n",
    "elif MODEL_NAME == 'EfficientNetB7':\n",
    "    eval = model.evaluate(test_generator)  # [test_loss, test_accuracy, test_precision, test_recall]\n",
    "\n",
    "    y_pred = np.argmax(model.predict(test_generator), axis=-1)\n",
    "    y_true = test_generator.labels\n",
    "    np.save(f\"{TEST_RESULT_FILE_PATH}/y_pred.npy\", y_pred)\n",
    "    np.save(f\"{TEST_RESULT_FILE_PATH}/y_true.npy\", y_true)\n",
    "\n",
    "    conf_mat = confusion_matrix(y_true, y_pred)\n",
    "    df_conf_mat = pd.DataFrame(conf_mat, columns=[str(i) for i in range(conf_mat.shape[0])],\n",
    "                               index=[str(i) for i in range(conf_mat.shape[1])])\n",
    "    sns_heatmap = sns.heatmap(data=df_conf_mat, annot=True, fmt='d', linewidths=.5, cmap='BuGn_r')\n",
    "    sns_heatmap.get_figure().savefig(f\"{TEST_RESULT_FILE_PATH}/confusion_matrix.png\")\n",
    "\n",
    "    target_names = [str(i) for i in range(conf_mat.shape[0])]\n",
    "    report = classification_report(y_true, y_pred, digits=5, target_names=target_names)\n",
    "\n",
    "    with open(f\"{TEST_RESULT_FILE_PATH}/result.txt\", \"w\") as file:\n",
    "        file.write(f\"test_loss: {eval[0]}, test_accuracy: {eval[1]}, test_precision: {eval[2]}, test_recall: {eval[3]}\\n\")\n",
    "        file.write(report)\n",
    "    print(f'test_loss: {eval[0]}, test_accuracy: {eval[1]}, test_precision: {eval[2]}, test_recall: {eval[3]}')\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
